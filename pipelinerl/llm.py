import asyncio
import datetime
import json
import logging
import os
import pprint
import time
from abc import ABC, abstractmethod
from collections import defaultdict
from statistics import mean
from typing import Any, Generator, Type, TypeAlias
from uuid import uuid4

import aiohttp
import jsonref
import litellm
import numpy as np
import requests
import transformers
from omegaconf import DictConfig, OmegaConf
from pydantic import BaseModel, Field, TypeAdapter
from tenacity import retry, stop_after_attempt, wait_exponential

logger = logging.getLogger(__name__)

PIPELINERL_LLM_TOKEN = "PIPELINERL_LLM_TOKEN"

class TrainingText(BaseModel):
    """
    Training text instance used to finetune a language model.

    Attributes:
        text (str): The full text of the training instance.
        n_predicted (int): The number of predicted characters in the text.
        reward (float): The reward associated with the training instance. Defaults to 0.0.
        logprobs (List[float]): A list of log probabilities of the completion tokens from the assistant model.
        ref_logprobs (List[float]): A list of reference log probabilities of the completion tokens from the reference model.
        input_ids (List[int]): The tokenized input ids of the text.
        labels (List[int]): The tokenized labels of the text (i.e., masked token ids for the prompt and regular token ids for the prediction).
        group_id (str, optional): ID of the group. It is used by the RL finetuning script to normalize rewards.
        prompt_text (str): Portion of the text that serves as the prompt (i.e., the text excluding the predicted characters).
        output_text (str): Portion of the text that represents the predicted output (i.e., the last n_predicted characters).
    """

    text: str
    n_predicted: int
    reward: float = 0.0
    logprobs: list[float] = Field(default_factory=list)
    ref_logprobs: list[float] = Field(default_factory=list)
    input_ids: list[int] = Field(default_factory=list)
    labels: list[int] = Field(default_factory=list)
    group_id: str | None = None
    metadata: dict = Field(default_factory=dict)

    @property
    def prompt_text(self) -> str:
        return self.text[: -self.n_predicted]

    @property
    def output_text(self) -> str:
        return self.text[-self.n_predicted :]


class Prompt(BaseModel):
    """
    A class representing a LLM prompt with messages and tools.

    Attributes:
        id (str): A unique identifier for the prompt, generated by default.
        tools (list[dict], optional): A list of tools associated with the prompt, default is None.
        messages (list[dict]): A list of messages in the prompt, default is an empty list.
    """

    id: str = Field(default_factory=lambda: str(uuid4()))
    tools: list[dict] | None = None
    messages: list[dict] = Field(default_factory=list)
    token_ids: list[int] = Field(default_factory=list)
    response_format: dict | Type[BaseModel] | None = None

    @staticmethod
    def from_user_message(content: str):
        """
        Creates a Prompt instance from a user message.

        Args:
            content (str): The content of the user message.

        Returns:
            Prompt: A Prompt instance with the user message.
        """
        return Prompt(messages=[{"role": "user", "content": content}])

    def __bool__(self) -> bool:
        return bool(self.messages)


LLMOutput: TypeAlias = litellm.utils.Message
"""Type alias for the output of the language model."""

class TokenLogprob(BaseModel):
    logprob: float
    token_id: int


class LLMCall(BaseModel):
    """
    LLMCall stores info about a call to a language model.

    Attributes:
        timestamp (str): The timestamp when the call was made, in ISO 8601 format.
        prompt (Prompt): The input prompt provided to the language model.
        output (LLMOutput): The output generated by the language model.
        prompt_length_tokens (int): The length of the prompt in tokens. Defaults to -1 if not set.
        output_length_tokens (int): The length of the output in tokens. Defaults to -1 if not set.
        cached (bool): Indicates whether the result was retrieved from cache.
    """

    timestamp: str = Field(default_factory=lambda: datetime.datetime.now().isoformat())
    prompt: Prompt
    output: LLMOutput
    prompt_length_tokens: int = -1
    output_length_tokens: int = -1
    cached: bool
    llm_info: dict = {}
    cost: float = 0
    logprobs: list[TokenLogprob] = Field(default_factory=list, exclude=True)


class LLMEvent(BaseModel):
    """An event class representing either a chunk of LLM output or the final LLM output.

    This class encapsulates events that occur during LLM processing, handling both
    intermediate chunks of output and the final complete output.

    Attributes:
        chunk (str, optional): A partial text output from the LLM stream.
        output (LLMOutput, optional): The complete output from the LLM.
        llm_call (LLMCall, optional): The entire LLMCall object.
    """

    chunk: str | None = None
    output: LLMOutput | None = None
    llm_call: LLMCall | None = None


class LLMStream:
    """A wrapper class for LLM generators that provides convenient iteration and output extraction.

    This class wraps a generator that yields LLMEvents and provides methods to:

    - Iterate through events
    - Extract complete LLM output
    - Get the assistant's response text

    LLMStream stores the LLM call object when the generator yields it.

    Attributes:
        generator: Generator yielding LLMEvents or None if empty
        prompt: The prompt used to generate the LLM response:

    Raises:
        ValueError: When trying to iterate null stream, when no output is produced,
                   or when output is not an assistant message with content
    """

    def __init__(self, generator: Generator[LLMEvent, None, None] | None, prompt: Prompt):
        self.generator = generator
        self.prompt = prompt

    def __bool__(self):
        return self.generator is not None

    def __iter__(self):
        if self.generator is None:
            raise ValueError("can't iterate a null stream")
        return self

    def __next__(self) -> LLMEvent:
        if self.generator is None:
            raise StopIteration
        event = next(self.generator)
        if event.llm_call:
            self.llm_call = event.llm_call
        return event

    def get_output(self) -> LLMOutput:
        """Returns first LLMOutput found in events"""
        for event in self:
            if event.output:
                return event.output
        raise ValueError("LLM did not produce an output")

    def get_text(self) -> str:
        """Returns content of first assistant message found"""
        o = self.get_output()
        if not o.role == "assistant" or o.content is None:
            raise ValueError("LLM did not produce an assistant message")
        return o.content

    def get_llm_call(self) -> LLMCall:
        """Returns the LLMCall object"""
        for event in self:
            if event.llm_call:
                break
        return self.llm_call


class LLM(BaseModel, ABC):
    """
    An abstract base class representing a Language Learning Model (LLM).

    This class defines the interface for interacting with different LLM implementations.
    It handles basic LLM functionality like token counting, generation, and logging.

    Attributes:
        model_name (str): Name of the LLM model
        parameters (dict): Model-specific parameters for generation
        context_size (int): Maximum context size in tokens (default: 32000)
        tokenizer_name (str): Name of the tokenizer used
        tokenizer (Any): Tokenizer instance
        token_count (int): Running count of tokens processed


    Note:
        This is an abstract class and requires implementation of the abstract methods
        in derived classes.
    """

    model_name: str
    parameters: dict = {}
    context_size: int = 32000
    tokenizer_name: str = ""
    tokenizer: Any = None

    token_count: int = 0
    _log: list = []
    _stats: dict = defaultdict(list)

    @abstractmethod
    def generate(self, prompt: Prompt, **kwargs) -> LLMStream:
        """
        Generate text from a given prompt

        Args:
            prompt (Prompt): The prompt object containing messages to send to the LLM.
            **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.

        Returns:
            LLMStream: A stream of LLM events containing the model's response.
        """
        pass

    @abstractmethod
    def count_tokens(self, messages: list[dict] | str) -> int:
        """
        Count tokens in messages or text

        Args:
            messages (Union[List[Dict], str]): List of messages or text to count tokens in

        Returns:
            int: Number of tokens in the messages or text
        """
        pass

    @abstractmethod
    def make_training_text(self, prompt: Prompt, output: LLMOutput) -> TrainingText:
        """
        Create training text from prompt and output.

        Args:
            prompt (Prompt): The prompt object containing messages used to generate the output.
            output (LLMOutput): The output generated by the LLM.

        Returns:
            TrainingText: The training text object containing the prompt and output.
        """
        pass

    def quick_response(self, text_prompt: str) -> str:
        prompt = Prompt(messages=[{"role": "user", "content": text_prompt}])
        outputs = []
        for e in self.generate(prompt):
            if e.output:
                outputs.append(e.output.content)
        return "".join(outputs)

    async def agenerate(self, prompt: Prompt, session: Any, **kwargs):
        raise NotImplementedError(f"asynchronous generation is not implemented in {self.__class__}")

    def get_info(self) -> dict:
        parameters = {
            k: (OmegaConf.to_container(v) if isinstance(v, DictConfig) else v) for k, v in self.parameters.items()
        }
        return {
            "model_name": self.model_name,
            "parameters": parameters,
            "context_size": self.context_size,
        }

    def get_token_costs(self) -> dict:
        """Returns prices for different kinds of tokens.

        See `result['input']` for the price of input tokens and
        `result['output']` for the price of output tokens respectively.
        """
        return {"input": 0, "output": 0}

    def log_output(
        self,
        prompt: Prompt,
        message: LLMOutput,
        cached: bool = False,
        count_tokens: bool = True,
        prompt_length_tokens: int = -1,
        output_length_tokens: int = -1,
    ) -> LLMCall:
        """
        Logs the output of an LLM (Language Model) call along with its metadata.

        Args:
            prompt (Prompt): The prompt object containing the input messages for the LLM.
            message (LLMOutput): The output message generated by the LLM.
            cached (bool, optional): Indicates whether the output was retrieved from cache. Defaults to False.
            prompt_length_tokens (int, optional): The length of the prompt in tokens. Defaults to -1.
            output_length_tokens (int, optional): The length of the output in tokens. Defaults to -1.

        Returns:
            LLMCall: An object containing the metadata of the LLM call, including prompt, output, and token costs.
        """

        start_log_output = time.perf_counter()
        if count_tokens:
            prompt_length_tokens = self.count_tokens(prompt.messages)
            if message.content:
                output_length_tokens = (
                    self.count_tokens(prompt.messages + [{"role": "assistant", "content": message.content}])
                    - prompt_length_tokens
                )
            else:
                output_length_tokens = 0
            self._stats["prompt_length_tokens"].append(prompt_length_tokens)
            self._stats["output_length_tokens"].append(output_length_tokens)

        llm_call = LLMCall(
            timestamp=datetime.datetime.now().isoformat(),
            prompt=prompt,
            output=message,
            prompt_length_tokens=prompt_length_tokens,
            output_length_tokens=output_length_tokens,
            cached=cached,
            llm_info=self.get_info(),
        )
        token_costs = self.get_token_costs()
        llm_call.cost = (
            token_costs["input"] * llm_call.prompt_length_tokens + token_costs["output"] * llm_call.output_length_tokens
        )
        self._log.append(llm_call.model_dump())
        time_log_output = time.perf_counter() - start_log_output
        self._stats["time_log_output"].append(time_log_output)
        return llm_call

    def get_stats(self) -> dict:
        return {
            "time_send_request": mean(self._stats["time_send_request"]) if self._stats["time_send_request"] else 0,
            "time_log_output": mean(self._stats["time_log_output"]) if self._stats["time_log_output"] else 0,
            "total_prompt_tokens": sum(self._stats["prompt_length_tokens"])
            if self._stats["prompt_length_tokens"]
            else 0,
            "total_output_tokens": sum(self._stats["output_length_tokens"])
            if self._stats["output_length_tokens"]
            else 0,
            "time_postprocess_llm_response": np.mean(self._stats["time_postprocess_llm_response"])
            if self._stats["time_postprocess_llm_response"]
            else 0,
        }

    def get_step_schema(self, cls):
        raise NotImplementedError("get_step_schema method not implemented")


class TrainableLLM(LLM):
    """
    Class for interacting with trainable language models through OpenAI-compatible API endpoints.

    This class implements functionality for both inference and training-related operations with
    language models served via Text Generation Inference (TGI) or vLLM endpoints that expose
    an OpenAI-compatible API interface. It supports streaming non-streaming and async modes
    and includes methods for token counting and log probability calculations.

    Attributes:
        base_url (str): Base URL of the API endpoint
        api_token (str): Authentication token for API access
    """

    # TODO: use OpenAI Python client when the certificate issue is resolved.
    # TODO: consider using litellm

    base_url: str = "https://api.openai.com"
    api_token: str = Field(default="", exclude=True)
    collect_logprobs: bool = False
    use_litellm_tokenizer_fallback: bool = False
    max_parallel_requests: int = 32
    max_retries: int = 5
    base_delay: float = 0.5
    _semaphore: asyncio.Semaphore

    def model_post_init(self, __context):
        super().model_post_init(__context)
        self.api_token = os.getenv(PIPELINERL_LLM_TOKEN, "") or os.getenv("OPENAI_API_KEY", "")
        self._semaphore = asyncio.Semaphore(self.max_parallel_requests)

    def get_base_url(self) -> str:
        """
        Returns the base URL for the API endpoint.
        """
        return self.base_url.rstrip("/")

    def parse_completion_logprobs(self, completion_logprobs: list[dict]) -> list[TokenLogprob]:
        logprobs = []
        for logprob in completion_logprobs:
            if logprob:
                try:
                    # We assume that the server was launched with --return-tokens-as-token-ids
                    # and that the tokens are provided as: ['token_id:1271', 'token_id:1505', '
                    logprobs.append(
                        TokenLogprob(
                            token_id=int(logprob["token"].split(":")[-1]),
                            logprob=logprob["logprob"],
                        )
                    )
                except Exception as e:
                    logger.error(f"Failed to process logprobs: {logprob}")
                    logger.error(e)

        return logprobs

    def generate(self, prompt: Prompt, **kwargs) -> LLMStream:
        """Generate a response stream from the language model based on the given prompt.

        This method handles both cached and new responses, implementing a caching mechanism
        for LLM responses to avoid redundant API calls.

        Args:
            prompt (Prompt): The prompt object containing messages to send to the LLM.
            **kwargs (dict, optional): Additional arguments to pass to the underlying LLM implementation.

        Returns:
            LLMStream: A stream of LLM events containing the model's response.

        Raises:
            ValueError: If cache miss occurs when replay mode is enabled (_REPLAY_SQLITE is True).

        Notes:
            - If caching is enabled and the prompt exists in cache, returns cached response
            - If generating new response, tokens are counted and added to total token count
            - All generated events are cached for future use if caching is enabled
            - Output is logged through the logging system
        """

        def _implementation():
            toks = self.count_tokens(prompt.messages)
            self.token_count += toks
            logger.debug(f"{toks} prompt tokens, total: {self.token_count}")
            for event in self._generate(prompt, **kwargs):
                yield event

        return LLMStream(_implementation(), prompt)


    @retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2))
    def _generate(self, prompt: Prompt) -> Generator[LLMEvent, None, None]:
        self.load_tokenizer()
        headers = {"Content-Type": "application/json"}
        if self.api_token:
            headers |= {"Authorization": f"Bearer {self.api_token}"}
        data = {
            "model": self.model_name,
            "messages": prompt.messages,
            "tools": prompt.tools,
        }
        if self.collect_logprobs:
            data.update(
                {
                    "logprobs": 1,
                    "include_stop_str_in_output": True,
                    "skip_special_tokens": False,
                }
            )
        logger.info(f"POST request to LLM {self.base_url}/v1/chat/completions")
        start_send_request = time.time()
        for k, v in self.parameters.items():
            data[k] = OmegaConf.to_container(v) if isinstance(v, DictConfig) else v
        r = requests.post(
            url=f"{self.base_url}/v1/chat/completions",
            json=data,
            headers=headers,
            verify=False,
        )
        time_send_request = time.time() - start_send_request
        logger.info(f"LLM request done in {time_send_request:.2f}s")
        self._stats["time_send_request"].append(time_send_request)
        if not r.ok:
            logger.error(f"Failed to get completion: {r.text}")
            r.raise_for_status()
        parsed_logprobs = []
        data = r.json()
        try:
            content = data["choices"][0]["message"]["content"]
            tool_calls = data["choices"][0]["message"].get("tool_calls", [])
            if not content and not tool_calls:
                logger.warning(f"Empty completion {data}")

            if self.collect_logprobs:
                completion_logprobs = data["choices"][0]["logprobs"]["content"]
                parsed_logprobs = self.parse_completion_logprobs(completion_logprobs)
        except Exception as e:
            logger.exception(f"Failed to parse llm response: {r}")
            raise e
        output = LLMOutput(content=content)
        if tool_calls:
            output.tool_calls = [litellm.ChatCompletionMessageToolCall(**tc) for tc in tool_calls]
        llm_call = self.log_output(prompt, output)
        llm_call.logprobs = parsed_logprobs
        yield LLMEvent(output=output, llm_call=llm_call)

    def get_step_schema(self, cls, simplify: bool = True) -> str:
        schema = TypeAdapter(cls).json_schema()
        dereferenced_schema: dict = dict(jsonref.replace_refs(schema, proxies=False))  # type: ignore
        clean_schema = []
        for step in dereferenced_schema["oneOf"]:
            step["properties"] = without(step["properties"], "metadata")
            if simplify:
                step = without(step, "title")
                for prop in step["properties"]:
                    step["properties"][prop] = without(step["properties"][prop], "title")
                step["properties"]["kind"] = {"const": step["properties"]["kind"]["const"]}
            clean_schema.append(step)
        return json.dumps(clean_schema, ensure_ascii=False)


    def load_tokenizer(self):
        """
        Loads the tokenizer for the model.

        If the tokenizer is not already loaded, this method will import the
        `transformers` library and load the tokenizer using the model name or
        tokenizer name.

        Raises:
            ValueError: If neither `self.tokenizer_name` nor `self.model_name` is provided
        """
        if self.tokenizer is None:
            global transformers
            if transformers is None:
                import transformers
            name = self.tokenizer_name or self.model_name
            try:
                self.tokenizer = transformers.AutoTokenizer.from_pretrained(name)
            except Exception as e:
                if not self.use_litellm_tokenizer_fallback:
                    raise e

    def make_training_text(self, prompt: Prompt, output: LLMOutput) -> TrainingText:
        """
        Generates training text from a given prompt and LLM output.

        This method loads the tokenizer and uses it to create training text
        suitable for training a language model.

        Args:
            prompt (Prompt): The input prompt to generate training text from.
            output (LLMOutput): The output from the language model to be used in training.

        Returns:
            TrainingText: The generated training text.
                - text (str): The formatted conversation text (prompt + output)
                - n_predicted (int): Length of the output text portion

        Note:
            - Uses tokenizer's chat template to format conversations
            - Removes BOS token if present in the beginning of the text
        """
        self.load_tokenizer()
        prompt_text = self.tokenizer.apply_chat_template(
            conversation=prompt.messages, tokenize=False, add_generation_prompt=True
        )
        text = self.tokenizer.apply_chat_template(
            prompt.messages + [{"role": "assistant", "content": output.content}],
            tokenize=False,
        )
        output_text = text[len(prompt_text) :]

        if self.tokenizer.bos_token and text.startswith(self.tokenizer.bos_token):
            text = text[len(self.tokenizer.bos_token) :]

        return TrainingText(text=text, n_predicted=len(output_text))

    def get_logprobs_token_ids(self, prompt_token_ids: list[int], completion_token_ids: list[int]) -> dict[str, Any]:
        if not self.tokenizer:
            self.load_tokenizer()

        headers = {"Content-Type": "application/json"}
        if self.api_token:
            headers |= {"Authorization": f"Bearer {self.api_token}"}

        generation_args = {
            "model": self.model_name,
            "prompt": prompt_token_ids + completion_token_ids,
            "temperature": 0.0,
            "max_tokens": 0,
            "logprobs": 0,
            "echo": True,
            "include_stop_str_in_output": True,  # self.include_stop_str_in_output,
            "skip_special_tokens": False,
            "n": 1,  # number of completions to generate
            "stream": False,  # return a single completion and not a stream of lines
        }
        url = f"{self.base_url}/v1/completions"
        logger.debug(f"POST request to {url}")
        r = requests.post(url, json=generation_args, headers=headers, verify=False)
        r.raise_for_status()  # raise exception if status code is not in the 200s
        try:
            response = r.json()
        except Exception as e:
            raise RuntimeError(f"Generation API wrong response: {r.text}", e)
        logprobs = []
        completion_logprobs = response["choices"][0]["prompt_logprobs"][-len(completion_token_ids) :]
        for lp in completion_logprobs:
            if lp:
                for k, v in lp.items():
                    v.update({"generated": 0, "token_id": k})
                    logprobs.append(v)
        return {"content": logprobs}

    def get_batch_logprobs_token_ids(
        self, prompt_token_ids: list[list[int]], completion_token_ids: list[list[int]]
    ) -> list[dict[str, Any]]:
        if not self.tokenizer:
            self.load_tokenizer()
        batch_size = len(prompt_token_ids)

        headers = {"Content-Type": "application/json"}
        if self.api_token:
            headers |= {"Authorization": f"Bearer {self.api_token}"}

        generation_args = {
            "model": self.model_name,
            "prompt": [pids + cids for pids, cids in zip(prompt_token_ids, completion_token_ids)],
            "temperature": 0.0,
            "max_tokens": 0,
            "logprobs": 0,
            "echo": True,
            "include_stop_str_in_output": True,  # self.include_stop_str_in_output,
            "skip_special_tokens": False,
            "n": 1,  # number of completions to generate
            "stream": False,  # return a single completion and not a stream of lines
        }
        url = f"{self.base_url}/v1/completions"
        logger.debug(f"POST request to {url}")
        r = requests.post(url, json=generation_args, headers=headers, verify=False)
        r.raise_for_status()

        try:
            response = r.json()
        except Exception as e:
            raise RuntimeError(f"Generation API wrong response: {r.text}", e)

        all_logprobs = []
        for i in range(batch_size):
            logprobs = []
            for lp in response["choices"][i]["prompt_logprobs"][-len(completion_token_ids[i]) :]:
                if lp:
                    for k, v in lp.items():
                        v.update({"generated": 0, "token_id": k})
                        logprobs.append(v)
            all_logprobs.append({"content": logprobs})
        return all_logprobs

    def get_logprobs_complete(self, prompt: str, output: str) -> dict[str, Any]:
        """
        Get the log probabilities of the tokens in the output given the prompt.

        This method sends a request to the language model API to generate the log probabilities
        for the tokens in the provided output, given the prompt. It uses the tokenizer to encode
        the prompt and output, and extracts the log probabilities from the API response.

        Args:
            prompt (str): The input prompt text.
            output (str): The output text for which log probabilities are to be calculated.

        Returns:
            list[float]: A list of log probabilities for each token in the output.

        Raises:
            RuntimeError: If the API response is not as expected or if there is a mismatch
                          between the tokens in the response and the provided output.
        """
        if not self.tokenizer:
            self.load_tokenizer()

        headers = {"Content-Type": "application/json"}
        if self.api_token:
            headers |= {"Authorization": f"Bearer {self.api_token}"}

        if self.tokenizer.bos_token and prompt.startswith(self.tokenizer.bos_token):
            prompt = prompt[len(self.tokenizer.bos_token) :]

        prompt_text = prompt + output
        generation_args = {
            "model": self.model_name,
            "prompt": prompt_text,
            "temperature": 0.0,
            "max_tokens": 0,
            "logprobs": 1,
            "echo": True,
            "include_stop_str_in_output": True,  # self.include_stop_str_in_output,
            "skip_special_tokens": False,
            "n": 1,  # number of completions to generate
            "stream": False,  # return a single completion and not a stream of lines
        }
        base_url = self.get_base_url()
        url = f"{base_url}/v1/completions"
        logger.debug(f"POST request to {url}")
        r = requests.post(url, json=generation_args, headers=headers, verify=False)
        r.raise_for_status()  # raise exception if status code is not in the 200s
        try:
            response = r.json()
            tokens = response["choices"][0]["logprobs"]["tokens"]
            log_probs = response["choices"][0]["logprobs"]["token_logprobs"]
            prompt_encoded = self.tokenizer.encode(prompt, add_special_tokens=True)
            prompt_completion_encoded = self.tokenizer.encode(prompt + output, add_special_tokens=True)
            completion_log_probs = log_probs[len(prompt_encoded) : len(prompt_completion_encoded)]
            completion_tokens = tokens[len(prompt_encoded) : len(prompt_completion_encoded)]
            assert (
                "".join(completion_tokens) == output
            ), f"Tokens do not match completion: {''.join(completion_tokens)} != {output}"
        except Exception as e:
            raise RuntimeError(f"Generation API wrong response: {r.text}", e)
        completion_log_probs = [
            {
                "logprob": lp,
                "top_logprobs": [],
                "token": t,
            }
            for lp, t in zip(completion_log_probs, completion_tokens)
        ]
        return {"content": completion_log_probs}

    def get_logprobs_chat_complete(self, prompt: Prompt, output: LLMOutput) -> dict[str, Any]:
        """
        Calculate the log probabilities of the tokens in the completion generated by the language model.

        This function sends a request to the language model API to generate completions and calculate log probabilities.
        The function uses the tokenizer to encode the prompt and completion texts.
        The log probabilities are extracted from the API response and validated against the original completion.

        Args:
            prompt (Prompt): The prompt containing the messages to be sent to the language model.
            output (LLMOutput): The output from the language model containing the generated completion.

        Returns:
            list[float]: A list of log probabilities for each token in the generated completion.

        Raises:
            RuntimeError: If the response from the generation API is incorrect or cannot be parsed.
        """
        headers = {"Content-Type": "application/json"}
        if self.api_token:
            headers |= {"Authorization": f"Bearer {self.api_token}"}

        time_t0 = time.time()
        prompt_text = self.tokenizer.apply_chat_template(prompt.messages, tokenize=False)
        completion = output.content or ""
        messages = prompt.messages + [{"role": "assistant", "content": completion}]
        prompt_text = self.tokenizer.apply_chat_template(prompt.messages, tokenize=False, add_generation_prompt=True)
        prompt_completion_text = self.tokenizer.apply_chat_template(messages, tokenize=False)
        if self.tokenizer.bos_token and prompt_text.startswith(self.tokenizer.bos_token):
            prompt_text = prompt_text[len(self.tokenizer.bos_token) :]
            prompt_completion_text = prompt_completion_text[len(self.tokenizer.bos_token) :]

        prompt_encoded = self.tokenizer.encode(prompt_text, add_special_tokens=False)
        prompt_completion_encoded = self.tokenizer.encode(prompt_completion_text, add_special_tokens=False)

        generation_args = {
            "model": self.model_name,
            "messages": messages,
            "temperature": 0.0,
            "max_tokens": 1,
            "logprobs": 1,
            "echo": True,
            "include_stop_str_in_output": True,  # self.include_stop_str_in_output,
            "skip_special_tokens": False,
            "n": 1,  # number of completions to generate
            "stream": False,  # return a single completion and not a stream of lines
        }
        base_url = self.get_base_url()
        r = requests.post(
            url=f"{base_url}/v1/chat/completions",
            json=generation_args,
            headers=headers,
            verify=False,
        )
        r.raise_for_status()

        try:
            response = r.json()
            log_probs = [list(log_prob.values())[0] for log_prob in response["prompt_logprobs"] if log_prob]
            completion_log_probs = log_probs[len(prompt_encoded) : len(prompt_completion_encoded)]
            decoded_completion_tokens = [log_prob["decoded_token"] for log_prob in completion_log_probs]
            reconstructed_completion = "".join(decoded_completion_tokens)
            if self.tokenizer.eos_token in reconstructed_completion:
                reconstructed_completion = reconstructed_completion[: -len(self.tokenizer.eos_token)]
            assert (
                reconstructed_completion == completion
            ), f"Tokens do not match completion: {reconstructed_completion} != {completion}"
        except Exception as e:
            raise RuntimeError(f"Generation API wrong response: {r.text}", e)

        logger.debug(f"Log likelihood calculation took {time.time() - time_t0:.2f} seconds")
        logger.debug(f"Tokens per second: {len(log_probs) / (time.time() - time_t0):.2f}")

        completion_log_probs = [
            {
                "logprob": o["logprob"],
                "top_logprobs": [],
                "token": o["decoded_token"],
            }
            for o in completion_log_probs
        ]

        return {"content": completion_log_probs}

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2))
    def get_logprobs(self, prompt: str | Prompt | list[int], output: str | LLMOutput | list[int]) -> dict[str, Any]:
        """
        Calculate the log probabilities of the given output based on the provided prompt.

        Args:
            prompt (Union[str, Prompt]): The input prompt, which can be either a string or a Prompt object.
            output (Union[str, LLMOutput]): The output to evaluate, which can be either a string or an LLMOutput object.

        Returns:
            list[float]: A list of log probabilities corresponding to the given output.

        Raises:
            ValueError: If the input types are not valid.
        """
        if isinstance(prompt, str) and isinstance(output, str):
            return self.get_logprobs_complete(prompt=prompt, output=output)
        elif isinstance(prompt, Prompt) and isinstance(output, LLMOutput):
            return self.get_logprobs_chat_complete(prompt=prompt, output=output)
        elif isinstance(prompt, list) and isinstance(output, list):
            return self.get_logprobs_token_ids(prompt_token_ids=prompt, completion_token_ids=output)
        else:
            raise ValueError("Invalid input types")

    def count_tokens(self, messages: list[dict] | str) -> int:
        """
        Count the number of tokens in the given messages.

        This method loads the tokenizer and then counts the number of tokens
        in the provided messages. The messages can be either a string or a list
        of dictionaries.

        Args:
            messages (Union[list[dict], str]): The messages to count tokens for. It can
                               be a single string or a list of dictionaries.

        Returns:
            int: The number of tokens in the provided messages.
        """
        try:
            self.load_tokenizer()
            if isinstance(messages, str):
                return len(self.tokenizer(messages).input_ids)
            else:
                add_generation_prompt = False if messages[-1]["role"] == "assistant" else True
                return len(self.tokenizer.apply_chat_template(messages, add_generation_prompt=add_generation_prompt))
        except Exception as e:
            if self.use_litellm_tokenizer_fallback:
                logger.warning("Failed to count tokens with tokenizer, fallback to litellm counter")
                if isinstance(messages, str):
                    return litellm.token_counter(model=self.model_name, text=messages)  # type: ignore
                else:
                    return litellm.token_counter(model=self.model_name, messages=messages)  # type: ignore
            else:
                raise e

    async def agenerate(self, prompt: Prompt, session: aiohttp.ClientSession, **kwargs) -> LLMStream:
        headers = {"Content-Type": "application/json"}
        if self.api_token:
            headers |= {"Authorization": f"Bearer {self.api_token}"}
        params = {"model": self.model_name, "messages": prompt.messages, "tools": prompt.tools} | kwargs
        for k, v in self.parameters.items():
            params[k] = OmegaConf.to_container(v) if isinstance(v, DictConfig) else v
        if self.collect_logprobs:
            params.update(
                {
                    "logprobs": 1,
                    "include_stop_str_in_output": True,
                    "skip_special_tokens": False,
                }
            )

        logger.debug(
            f"POST request to {self.base_url}/v1/chat/completions with params: {pprint.pformat(params, width=120)}"
        )
        async with self._semaphore:  # type: ignore
            retry_count = 0
            while True:
                try:
                    async with session.post(
                        url=f"{self.base_url}/v1/chat/completions", json=params, headers=headers, ssl=False
                    ) as response:
                        if not response.ok:
                            error_text = await response.text()
                            logger.error(f"Failed to get completion: {error_text}")
                            response.raise_for_status()
                        data = await response.json()
                        break
                except asyncio.TimeoutError as e:
                    logger.exception("API Timeout, retrying in 1 sec")
                    retry_count += 1
                    if retry_count > self.max_retries:
                        raise e
                    delay = self.base_delay * (2 ** (retry_count - 1))
                    logger.warning(
                        f"API Timeout, retrying in {delay:.2f} seconds (attempt {retry_count}/{self.max_retries})"
                    )
                    await asyncio.sleep(delay)
                except aiohttp.ClientError as e:
                    logger.error(f"Connection error for {self.base_url}/v1/chat/completions: {e}")
                    retry_count += 1
                    if retry_count > self.max_retries:
                        raise e
                    delay = self.base_delay * (2 ** (retry_count - 1))
                    logger.warning(f"Retrying in {delay:.2f} seconds (attempt {retry_count}/{self.max_retries})")
                    await asyncio.sleep(delay)

        try:
            content = data["choices"][0]["message"]["content"]
            tool_calls = data["choices"][0]["message"].get("tool_calls", [])
            if not content and not tool_calls:
                logger.warning(f"Empty completion {data}")

            parsed_logprobs = []
            if self.collect_logprobs:
                completion_logprobs = data["choices"][0]["logprobs"]["content"]
                for logprob in completion_logprobs:
                    if logprob:
                        try:
                            # We assume that the server was launched with --return-tokens-as-token-ids
                            # and that the tokens are provided as: ['token_id:1271', 'token_id:1505', '
                            parsed_logprobs.append(
                                TokenLogprob(
                                    token_id=int(logprob["token"].split(":")[-1]),
                                    logprob=logprob["logprob"],
                                )
                            )
                        except Exception as e:
                            logger.error(f"Failed to process logprobs: {logprob}")
                            logger.error(e)
        except Exception as e:
            logger.exception(f"Failed to parse llm response: {data}")
            raise e

        prompt_tokens = data["usage"]["prompt_tokens"]
        completion_tokens = data["usage"]["completion_tokens"]

        output = LLMOutput(content=content or "")
        if tool_calls:
            output.tool_calls = [litellm.ChatCompletionMessageToolCall(**tc) for tc in tool_calls]
        if not isinstance(prompt, Prompt):
            prompt = Prompt(**prompt.model_dump(exclude_none=True)) # recreate the prompt object from the possible tapeagents prompt
        llm_call = self.log_output(
            prompt,
            output,
            prompt_length_tokens=prompt_tokens,
            output_length_tokens=completion_tokens,
            count_tokens=False,
        )
        assert llm_call is not None, "llm_call is None"
        llm_call.logprobs = parsed_logprobs

        def _gen():
            yield LLMEvent(llm_call=llm_call, output=output)

        return LLMStream(generator=_gen(), prompt=prompt)


def without(d: dict, key: str) -> dict:
    if key in d:
        d.pop(key)
    return d
